<div class='container-fluid'>
  <div class='row'>
    <div class='col-12'>
      <div class='text' style='width: 1000; text-align:left'>
        <h2>Benchmark Objective</h2>
        
        <p>Our goal in runnin this benchmark was to provide accurate, automated data on how well various voice platforms perform.</p>

        <p>This first test evaluates three platforms on how they do on Question and Answer queries.</p>

        <p>Our goal is to provide interesting insight into how the platforms work, as well as show off our technology and processes at Bespoken.</p>

        <p>To that end, we combine:</p>
          <ul style='margin-top'>
            <li><a href='https://bespoken.io/test-robot'>Our core testing and tuning technology</a> 
            <li><a href='https://github.com/bespoken/nlp-benchmark'>Github</a> for hosting and managing the test scripts
            <li><a href='https://github.com/bespoken/nlp-benchmark/actions?query=workflow%3Aprocess'>Github Actions</a> for executing the benchmark
            <li>MySQL and Metabase for reporting
          </ul>      
        <p>
          All of this is public and open-source, part of our effort to "test in public". We want to show off not just our results, but also how we do our testing. We believe both are critical to successful automation.
        </p>
        <h2>Benchmark Dataset</h2>
      
        <p>We leveraged the excellent <a href='http://qa.mpi-inf.mpg.de/comqa/'>ComQA</a> dataset for this, specifically their Dev dataset of questions. From their website:</p>
        
        <p class='quote'>ComQA is a dataset of 11,214 questions, which were collected from <a href='http://www.answers.com/'>WikiAnswers</a>, a community question answering website. By collecting questions from such a site we ensure that the information needs are ones of interest to actual users. Moreover, questions posed there are often cannot be answered by commercial search engines or QA technology, making them more interesting for driving future research compared to those collected from an engine's query log. The dataset contains questions with various challenging phenomena such as the need for temporal reasoning, comparison (e.g., comparatives, superlatives, ordinals), compositionality (multiple, possibly nested, subquestions with multiple entities), and unanswerable questions (e.g., Who was the first human being on Mars?).</p>
        
        <p><a href='https://www.aclweb.org/anthology/N19-1027.pdf'>In their paper</a>, they elaborate further on each of these question categories. For example:</p>
        
        <p class='quote'>A question is compositional if answering it requires answering more primitive questions and combining these. These can beintersection or nested questions. Intersection questions are ones where two or more subquestions can be answered independently, and their answers intersected (e.g., “Which films featuring Tom Hanks did Spielberg direct?”). In nested questions, the answer to one subquestion is necessary to answer another ("Who were the parents of the thirteenth president of the US?").</p>
        
        <p>Take a look at the detailed findings for the question types <a href='/questionTypes'>here</a> to see the results for the different platforms per question type, as well as in-depth explanations on each question type.</p>
        
        <h2>Benchmark Test Execution</h2>
        
        <p>This dataset comprises 966 questions, which we run against the following three platforms:
          <ul>
            <li>Amazon Echo Show 5</li>
            <li>Apple iPad Mini</li>
            <li>Google Nest Home Hub</li>
          </ul>
        </p>

        <p>We used our <a href='https://bespoken.io/test-robot'>Bespoken Test Robots</a> to "talk" with these devices and record their audio and visual responses. With our Test Robots we are able to execute these tests in completely automated manner.</p>
      </div>
    </div>
  </div>

</div>